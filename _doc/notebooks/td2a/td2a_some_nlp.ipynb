{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2A : Texte et machine learning\n",
        "\n",
        "Revue de m\u00e9thodes de [word embedding](https://en.wikipedia.org/wiki/Word_embedding) statistiques (~ [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)) ou comment transformer une information textuelle en vecteurs dans un espace vectoriel (*features*) ? Deux exercices sont ajout\u00e9s \u00e0 la fin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
              "<script>\n",
              "function repeat_indent_string(n){\n",
              "    var a = \"\" ;\n",
              "    for ( ; n > 0 ; --n) {\n",
              "        a += \"    \";\n",
              "    }\n",
              "    return a;\n",
              "}\n",
              "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item) {\n",
              "    var anchors = document.getElementsByClassName(\"section\");\n",
              "    if (anchors.length == 0) {\n",
              "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
              "    }\n",
              "    var i,t;\n",
              "    var text_menu = begin;\n",
              "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
              "    var ind = \"\";\n",
              "    var memo_level = 1;\n",
              "    var href;\n",
              "    var tags = [];\n",
              "    var main_item = 0;\n",
              "    for (i = 0; i <= llast; i++) {\n",
              "        tags.push(\"h\" + i);\n",
              "    }\n",
              "\n",
              "    for (i = 0; i < anchors.length; i++) {\n",
              "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
              "\n",
              "        var child = null;\n",
              "        for(t = 0; t < tags.length; t++) {\n",
              "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
              "            if (r.length > 0) {\n",
              "child = r[0];\n",
              "break;\n",
              "            }\n",
              "        }\n",
              "        if (child == null){\n",
              "            text_memo += \"null\\n\";\n",
              "            continue;\n",
              "        }\n",
              "        if (anchors[i].hasAttribute(\"id\")) {\n",
              "            // when converted in RST\n",
              "            href = anchors[i].id;\n",
              "            text_memo += \"#1-\" + href;\n",
              "            // passer \u00e0 child suivant (le chercher)\n",
              "        }\n",
              "        else if (child.hasAttribute(\"id\")) {\n",
              "            // in a notebook\n",
              "            href = child.id;\n",
              "            text_memo += \"#2-\" + href;\n",
              "        }\n",
              "        else {\n",
              "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
              "            continue;\n",
              "        }\n",
              "        var title = child.textContent;\n",
              "        var level = parseInt(child.tagName.substring(1,2));\n",
              "\n",
              "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
              "\n",
              "        if ((level < lfirst) || (level > llast)) {\n",
              "            continue ;\n",
              "        }\n",
              "        if (title.endsWith('\u00b6')) {\n",
              "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\")\n",
              "        }\n",
              "\n",
              "        if (title.length == 0) {\n",
              "            continue;\n",
              "        }\n",
              "\n",
              "        while (level < memo_level) {\n",
              "            text_menu += \"</ul>\\n\";\n",
              "            memo_level -= 1;\n",
              "        }\n",
              "        if (level == lfirst) {\n",
              "            main_item += 1;\n",
              "        }\n",
              "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
              "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
              "            continue;\n",
              "        }\n",
              "        while (level > memo_level) {\n",
              "            text_menu += \"<ul>\\n\";\n",
              "            memo_level += 1;\n",
              "        }\n",
              "        text_menu += repeat_indent_string(level-2) + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
              "    }\n",
              "    while (1 < memo_level) {\n",
              "        text_menu += \"</ul>\\n\";\n",
              "        memo_level -= 1;\n",
              "    }\n",
              "    text_menu += send;\n",
              "    //text_menu += \"\\n\" + text_memo;\n",
              "    return text_menu;\n",
              "};\n",
              "var update_menu = function() {\n",
              "    var sbegin = \"\";\n",
              "    var sformat = '<li><a href=\"#__HREF__\">__TITLE__</a></li>';\n",
              "    var send = \"\";\n",
              "    var keep_item = -1;\n",
              "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item);\n",
              "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
              "    menu.innerHTML=text_menu;\n",
              "};\n",
              "window.setTimeout(update_menu,2000);\n",
              "            </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from jyquickhelper import add_notebook_menu\n",
        "add_notebook_menu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Donn\u00e9es\n",
        "\n",
        "Nous allons travailler sur des donn\u00e9es twitter collect\u00e9es avec le mot-cl\u00e9 macron : [tweets_macron_sijetaispresident_201609.zip](https://github.com/sdpython/ensae_teaching_cs/raw/master/src/ensae_teaching_cs/data/data_web/tweets_macron_sijetaispresident_201609.zip)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <td>776066992054861825</td>\n",
              "      <td>776067660979245056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb_user_mentions</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb_extended_entities</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb_hashtags</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>geo</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text_hashtags</th>\n",
              "      <td>, SiJ\u00e9taisPr\u00e9sident</td>\n",
              "      <td>, SiJ\u00e9taisPr\u00e9sident</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annee</th>\n",
              "      <td>2016</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delimit_mention</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lang</th>\n",
              "      <td>fr</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_str</th>\n",
              "      <td>7.76067e+17</td>\n",
              "      <td>7.76068e+17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text_mention</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>retweet_count</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>favorite_count</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>type_extended_entities</th>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>#SiJ\u00e9taisPr\u00e9sident se serait la fin du monde.....</td>\n",
              "      <td>#SiJ\u00e9taisPr\u00e9sident je donnerai plus de vacance...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb_user_photos</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb_urls</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb_symbols</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>created_at</th>\n",
              "      <td>Wed Sep 14 14:36:04 +0000 2016</td>\n",
              "      <td>Wed Sep 14 14:38:43 +0000 2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delimit_hash</th>\n",
              "      <td>, 0, 18</td>\n",
              "      <td>, 0, 18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                        0  \\\n",
              "index                                                  776066992054861825   \n",
              "nb_user_mentions                                                        0   \n",
              "nb_extended_entities                                                    0   \n",
              "nb_hashtags                                                             1   \n",
              "geo                                                                   NaN   \n",
              "text_hashtags                                         , SiJ\u00e9taisPr\u00e9sident   \n",
              "annee                                                                2016   \n",
              "delimit_mention                                                       NaN   \n",
              "lang                                                                   fr   \n",
              "id_str                                                        7.76067e+17   \n",
              "text_mention                                                          NaN   \n",
              "retweet_count                                                           4   \n",
              "favorite_count                                                          3   \n",
              "type_extended_entities                                                 []   \n",
              "text                    #SiJ\u00e9taisPr\u00e9sident se serait la fin du monde.....   \n",
              "nb_user_photos                                                          0   \n",
              "nb_urls                                                                 0   \n",
              "nb_symbols                                                              0   \n",
              "created_at                                 Wed Sep 14 14:36:04 +0000 2016   \n",
              "delimit_hash                                                      , 0, 18   \n",
              "\n",
              "                                                                        1  \n",
              "index                                                  776067660979245056  \n",
              "nb_user_mentions                                                        0  \n",
              "nb_extended_entities                                                    0  \n",
              "nb_hashtags                                                             1  \n",
              "geo                                                                   NaN  \n",
              "text_hashtags                                         , SiJ\u00e9taisPr\u00e9sident  \n",
              "annee                                                                2016  \n",
              "delimit_mention                                                       NaN  \n",
              "lang                                                                   fr  \n",
              "id_str                                                        7.76068e+17  \n",
              "text_mention                                                          NaN  \n",
              "retweet_count                                                           5  \n",
              "favorite_count                                                          8  \n",
              "type_extended_entities                                                 []  \n",
              "text                    #SiJ\u00e9taisPr\u00e9sident je donnerai plus de vacance...  \n",
              "nb_user_photos                                                          0  \n",
              "nb_urls                                                                 0  \n",
              "nb_symbols                                                              0  \n",
              "created_at                                 Wed Sep 14 14:38:43 +0000 2016  \n",
              "delimit_hash                                                      , 0, 18  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ensae_teaching_cs.data import twitter_zip\n",
        "df = twitter_zip(as_df=True)\n",
        "df.head(n=2).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5088, 20)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5000 tweets n'est pas assez pour tirer des conclusions mais cela donne une id\u00e9e. On supprime les valeurs manquantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5087, 2)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = df[[\"retweet_count\", \"text\"]].dropna()\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construire une pond\u00e9ration\n",
        "\n",
        "Le texte est toujours d\u00e9licat \u00e0 traiter. Il n'est pas toujours \u00e9vident de sortir d'une information binaire : un mot est-il pr\u00e9sent ou pas. Les mots n'ont aucun sens num\u00e9rique. Une liste de tweets n'a pas beaucoup de sens \u00e0 part les trier par une autre colonne : les retweet par exemple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2038</th>\n",
              "      <td>842.0</td>\n",
              "      <td>#SiJetaisPresident travailler moins pour gagne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2453</th>\n",
              "      <td>816.0</td>\n",
              "      <td>#SiJetaisPresident je ferais revenir l'\u00e9t\u00e9 ave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2627</th>\n",
              "      <td>529.0</td>\n",
              "      <td>#SiJetaisPresident le mcdo livrerai \u00e0 domicile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1402</th>\n",
              "      <td>289.0</td>\n",
              "      <td>#SiJetaisPresident les devoirs \u00e7a serait de re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2198</th>\n",
              "      <td>276.0</td>\n",
              "      <td>#SiJetaisPresident ? Pr\u00e9sident c'est pour les...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      retweet_count                                               text\n",
              "2038          842.0  #SiJetaisPresident travailler moins pour gagne...\n",
              "2453          816.0  #SiJetaisPresident je ferais revenir l'\u00e9t\u00e9 ave...\n",
              "2627          529.0     #SiJetaisPresident le mcdo livrerai \u00e0 domicile\n",
              "1402          289.0  #SiJetaisPresident les devoirs \u00e7a serait de re...\n",
              "2198          276.0   #SiJetaisPresident ? Pr\u00e9sident c'est pour les..."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.sort_values(\"retweet_count\", ascending=False).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sans cette colonne qui mesure la popularit\u00e9, il faut trouver un moyen d'extraire de l'information. On d\u00e9coupe alors en mots et on constuire un mod\u00e8le de langage : les [n-grammes](https://fr.wikipedia.org/wiki/N-gramme). Si un tweet est constitu\u00e9 de la s\u00e9quence de mots $(m_1, m_2, ..., m_k)$. On d\u00e9finit sa probabilit\u00e9 comme :\n",
        "\n",
        "$$P(tweet) = P(w_1, w_2) P(w_3 | w_2, w_1) P(w_4 | w_3, w_2) ... P(w_k | w_{k-1}, w_{k-2})$$\n",
        "\n",
        "Dans ce cas, $n=3$ car on suppose que la probabilit\u00e9 d'apparition d'un mot ne d\u00e9pend que des deux pr\u00e9c\u00e9dents. On estime chaque n-grammes comme suit :\n",
        "\n",
        "$$P(c | a, b) = \\frac{ \\# (a, b, c)}{ \\# (a, b)}$$\n",
        "\n",
        "C'est le nombre de fois o\u00f9 on observe la s\u00e9quence $(a,b,c)$ divis\u00e9 par le nombre de fois o\u00f9 on observe la s\u00e9quence $(a,b)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenisation\n",
        "\n",
        "D\u00e9couper en mots para\u00eet simple ``tweet.split()`` et puis il y a toujours des surprises avec le texte, la prise en compte des tirets, les majuscules, les espaces en trop. On utilse un *tokenizer* d\u00e9di\u00e9 : [TweetTokenizer](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.casual.TweetTokenizer) ou un tokenizer qui prend en compte le langage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['#sij\u00e9taispr\u00e9sident',\n",
              " 'se',\n",
              " 'serait',\n",
              " 'la',\n",
              " 'fin',\n",
              " 'du',\n",
              " 'monde',\n",
              " '...',\n",
              " 'mdr',\n",
              " '\ud83d\ude02']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer(preserve_case=False)\n",
        "tokens = tknzr.tokenize(data.ix[0, \"text\"])\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n-grammes\n",
        "\n",
        "* [N-Gram-Based Text Categorization: Categorizing Text With Python](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(None, None, None, '#sij\u00e9taispr\u00e9sident'),\n",
              " (None, None, '#sij\u00e9taispr\u00e9sident', 'se'),\n",
              " (None, '#sij\u00e9taispr\u00e9sident', 'se', 'serait'),\n",
              " ('#sij\u00e9taispr\u00e9sident', 'se', 'serait', 'la'),\n",
              " ('se', 'serait', 'la', 'fin'),\n",
              " ('serait', 'la', 'fin', 'du'),\n",
              " ('la', 'fin', 'du', 'monde'),\n",
              " ('fin', 'du', 'monde', '...'),\n",
              " ('du', 'monde', '...', 'mdr'),\n",
              " ('monde', '...', 'mdr', '\ud83d\ude02'),\n",
              " ('...', 'mdr', '\ud83d\ude02', None),\n",
              " ('mdr', '\ud83d\ude02', None, None),\n",
              " ('\ud83d\ude02', None, None, None)]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "generated_ngrams = ngrams(tokens, 4, pad_left=True, pad_right=True)\n",
        "list(generated_ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 1 : calculer des n-grammes sur les tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nettoyage\n",
        "\n",
        "Tous les mod\u00e8les sont plus stables sans les stop-words, c'est-\u00e0-dire tous les mots pr\u00e9sents dans n'importe quel documents et qui n'apporte pas de sens (\u00e0, de, le, la, ...). Souvent, on enl\u00e8ve les accents, la ponctuation... Moins de variabilit\u00e9 signifie des statistiques plus fiable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 2 : nettoyer les tweets\n",
        "\n",
        "Voir [stem](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structure de graphe\n",
        "\n",
        "On cherche cette fois-ci \u00e0 construire des coordonn\u00e9es pour chaque tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### matrice d'adjacence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une option courante est de d\u00e9couper chaque expression en mots puis de cr\u00e9er une matrice *expression x mot* ou chaque case indique la pr\u00e9sence d'un mot dans une expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5087, 11924)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "counts = count_vect.fit_transform(data[\"text\"])\n",
        "counts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On aboutit \u00e0 une matrice sparse ou chaque expression est repr\u00e9sent\u00e9e \u00e0 une vecteur ou chaque 1 repr\u00e9sente l'appartenance d'un mot \u00e0 l'ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0]], dtype=int64)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts[:5,:5].toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'#SiJ\u00e9taisPr\u00e9sident se serait la fin du monde... mdr \ud83d\ude02'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.ix[0,\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts[0,:].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### td-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce genre de technique produit des matrices de tr\u00e8s grande dimension qu'il faut r\u00e9duire. On peut enlever les mots rares ou les mots tr\u00e8s fr\u00e9quents. [td-idf](https://fr.wikipedia.org/wiki/TF-IDF) est une technique qui vient des moteurs de recherche. Elle construit le m\u00eame type de matrice (m\u00eame dimension) mais associe \u00e0 chaque couple (document - mot) un poids qui d\u00e9pend de la fr\u00e9quence d'un mot globalement et du nombre de documents contenant ce mot.\n",
        "\n",
        "$$idf(t) = \\log \\frac{\\# D}{\\#\\{d \\; | \\; t \\in d \\}}$$\n",
        "\n",
        "O\u00f9 :\n",
        "\n",
        "* $\\#D$ est le nombre de tweets\n",
        "* $\\#\\{d \\; | \\; t \\in d \\}$ est le nombre de tweets contenant le mot $t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$f(t,d)$ est le nombre d'occurences d'un mot $t$ dans un document $d$.\n",
        "\n",
        "$$tf(t,d) = \\frac{1}{2} + \\frac{1}{2} \\frac{f(t,d)}{\\max_{t' \\in d} f(t',d)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On construit le nombre $tfidf(t,f)$\n",
        "\n",
        "$$tdidf(t,d) = tf(t,d) idf(t)$$\n",
        "\n",
        "Le terme $idf(t)$ favorise les mots pr\u00e9sent dans peu de documents, le terme $tf(t,f)$ favorise les termes r\u00e9p\u00e9t\u00e9s un grand nombre de fois dans le m\u00eame document. On applique \u00e0 la matrice pr\u00e9c\u00e9dente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5087, 11924)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer()\n",
        "res = tfidf.fit_transform(counts)\n",
        "res.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.6988143126521051"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res[0,:].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 3 : tf-idf sans mot-cl\u00e9s\n",
        "\n",
        "La matrice ainsi cr\u00e9\u00e9e est de grande dimension. Il faut trouver un moyen de la r\u00e9duire avec [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### word2vec\n",
        "\n",
        "* [word2vec From theory to practice](http://hen-drik.de/pub/Heuer%20-%20word2vec%20-%20From%20theory%20to%20practice.pdf)\n",
        "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
        "* [word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
        "\n",
        "Cet algorithme part d'une r\u00e9presentation des mots sous forme de vecteur en un espace de dimension N = le nombre de mots distinct. Un mot est repr\u00e9sent\u00e9 par $(0,0, ..., 0, 1, 0, ..., 0)$. L'astuce consiste \u00e0 r\u00e9duire le nombre de dimensions en compressant avec une ACP, un r\u00e9seau de neurones non lin\u00e9aires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['#sij\u00e9taispr\u00e9sident',\n",
              " 'se',\n",
              " 'serait',\n",
              " 'la',\n",
              " 'fin',\n",
              " 'du',\n",
              " 'monde',\n",
              " '...',\n",
              " 'mdr',\n",
              " '\ud83d\ude02']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [tknzr.tokenize(_) for _ in data[\"text\"]]\n",
        "sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2016-10-03 00:42:12,299 : INFO : collecting all words and their counts\n",
            "2016-10-03 00:42:12,299 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2016-10-03 00:42:12,327 : INFO : collected 13263 word types from a corpus of 76467 raw words and 5087 sentences\n",
            "2016-10-03 00:42:12,392 : INFO : min_count=1 retains 13263 unique words (drops 0)\n",
            "2016-10-03 00:42:12,392 : INFO : min_count leaves 76467 word corpus (100% of original 76467)\n",
            "2016-10-03 00:42:12,484 : INFO : deleting the raw counts dictionary of 13263 items\n",
            "2016-10-03 00:42:12,484 : INFO : sample=0.001 downsamples 46 most-common words\n",
            "2016-10-03 00:42:12,484 : INFO : downsampling leaves estimated 56079 word corpus (73.3% of prior 76467)\n",
            "2016-10-03 00:42:12,484 : INFO : estimated required memory for 13263 words and 100 dimensions: 17241900 bytes\n",
            "2016-10-03 00:42:12,559 : INFO : resetting layer weights\n",
            "2016-10-03 00:42:12,893 : INFO : training model with 3 workers on 13263 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5\n",
            "2016-10-03 00:42:12,893 : INFO : expecting 5087 sentences, matching count from corpus used for vocabulary survey\n",
            "2016-10-03 00:42:13,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2016-10-03 00:42:13,376 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2016-10-03 00:42:13,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2016-10-03 00:42:13,391 : INFO : training on 382335 raw words (280377 effective words) took 0.5s, 575841 effective words/s\n"
          ]
        }
      ],
      "source": [
        "import gensim, logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "model = gensim.models.Word2Vec(sentences, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2016-10-03 00:42:18,089 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('bien', 0.9993120431900024),\n",
              " ('mon', 0.9993098974227905),\n",
              " ('ministre', 0.9993038177490234),\n",
              " ('3', 0.9993023872375488),\n",
              " ('/', 0.9992935657501221),\n",
              " ('ma', 0.999293327331543),\n",
              " ('dans', 0.9992847442626953),\n",
              " ('merde', 0.9992846250534058),\n",
              " ('\u20ac', 0.9992778301239014),\n",
              " ('#bayrou', 0.9992730617523193)]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similar_by_word(\"fin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model[\"fin\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.07167088,  0.01037968, -0.0347001 ,  0.13618709, -0.04936545,\n",
              "        0.06919333, -0.07977331, -0.04044997, -0.10819082, -0.12842277,\n",
              "        0.08534056, -0.07823293, -0.09098279,  0.01913173, -0.23570576,\n",
              "        0.03220233,  0.0213078 ,  0.24803311,  0.03564203, -0.03161603,\n",
              "       -0.03495153, -0.07290805,  0.03283146, -0.00746943,  0.07626498,\n",
              "        0.08364875,  0.00777306, -0.07886092,  0.12784876,  0.07973223,\n",
              "        0.06004526,  0.05176223, -0.06843602, -0.1104833 , -0.10198253,\n",
              "        0.0043983 , -0.09931083, -0.05931143, -0.04970795, -0.02507968,\n",
              "       -0.24674726,  0.02844877,  0.23148981, -0.01295278,  0.01698331,\n",
              "       -0.09990757, -0.10728305, -0.09666982,  0.07170945,  0.06428482,\n",
              "       -0.06271251, -0.00600671,  0.20045315,  0.06947709,  0.19523463,\n",
              "        0.05860612, -0.00956623,  0.05660482, -0.04068514, -0.02396445,\n",
              "        0.10675795, -0.08158956, -0.01152411, -0.04703977,  0.09980039,\n",
              "        0.10392339, -0.07677256, -0.00409548,  0.06592534, -0.00480576,\n",
              "        0.00601762, -0.09637805,  0.0136368 , -0.07772852, -0.08013346,\n",
              "       -0.16120504, -0.03272396, -0.02562772,  0.07080878,  0.01316294,\n",
              "       -0.26966235,  0.0906961 , -0.16877754,  0.06177418,  0.09502917,\n",
              "       -0.05076392,  0.2041503 , -0.07089066,  0.09621479,  0.02082463,\n",
              "       -0.06236167,  0.09625127, -0.08881571,  0.19672391,  0.03073362,\n",
              "       -0.07315189,  0.0742472 , -0.03126399,  0.05871766,  0.13382684], dtype=float32)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model[\"fin\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tagging\n",
        "\n",
        "L'objectif est de tagger les mots comme d\u00e9terminer si un mot est un verbe, un adjectif ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### grammar\n",
        "\n",
        "Voir [html.grammar](http://www.nltk.org/api/nltk.html#module-nltk.grammar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CRF\n",
        "\n",
        "Voir [CRF](http://www.nltk.org/api/nltk.tag.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HMM\n",
        "\n",
        "Voir [HMM](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering\n",
        "\n",
        "Une fois qu'on a des coordonn\u00e9es, on peut faire plein de choses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA\n",
        "\n",
        "* [Latent Dirichlet Application](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
        "* [LatentDirichletAllocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=1000)\n",
        "tfidf = tfidf_vectorizer.fit_transform(data[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5087, 1000)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_topics=10, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "             evaluate_every=-1, learning_decay=0.7,\n",
              "             learning_method='online', learning_offset=50.0,\n",
              "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
              "             n_jobs=1, n_topics=10, perp_tol=0.1, random_state=0,\n",
              "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lda.fit(tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['avoir', 'bac', 'bah']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "tf_feature_names[100:103]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic #%d:\" % topic_idx)\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic #0:\n",
            "gratuit mcdo supprimerai \u00e9cole soir kebab macdo kfc domicile cc\n",
            "Topic #1:\n",
            "macron co https de la est le il et hollande\n",
            "Topic #2:\n",
            "sijetaispresident je les de la et le des en pour\n",
            "Topic #3:\n",
            "notaires eu organiserais mets carte nouveaux journ\u00e9es installation cache cr\u00e9er\n",
            "Topic #4:\n",
            "sijetaispresident interdirais les je ballerines la serait serais bah de\n",
            "Topic #5:\n",
            "ministre de sijetaispresident la je premier mort et nommerais pr\u00e9sident\n",
            "Topic #6:\n",
            "cours le supprimerais jour sijetaispresident lundi samedi semaine je vendredi\n",
            "Topic #7:\n",
            "port interdirait d\u00e9missionnerais promesses heure rendrai ballerine mes changement christineboutin\n",
            "Topic #8:\n",
            "seraient sijetaispresident gratuits aux les nos putain \u00e9ducation nationale bonne\n",
            "Topic #9:\n",
            "bordel seront l\u00e9galiserai putes gratuites pizza mot virerais vitesse dutreil\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_top_words(lda, tf_feature_names, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 4 : LDA\n",
        "\n",
        "Recommencer en supprimant les stop-words pour avoir des r\u00e9sultats plus propres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}